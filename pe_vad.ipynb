{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voice Activity Detection Using Permutation Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voice Activity Detection\n",
    "\n",
    "Voice activity detection is the process of determining areas of speech v. non-speech in an audio signal. Some researchers call this \"Speech Activity Detection\", and further refine Voice Activity Detection to differentiate voiced v. unvoiced parts of speech. I am not going to do this, even though one of the papers this notebook is based on does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation Entropy\n",
    "\n",
    "Permutation entropy is a measure of complexity for time series data. For most measures of complexity, noise must be removed from the signal for complexity measurements to be effective, but this can alter the signal as well as add computation time to the overall system, which slows things down. Permutation entropy avoids all this by defining a simple complexity measure. The best introduction to permutation entropy is with some examples.\n",
    "\n",
    "Given a series with seven values:\n",
    "\n",
    "$$x = (4, 7, 9, 10, 6, 11, 3)$$\n",
    "\n",
    "There are six pairs of neighbors,\n",
    "\n",
    "$$(4, 7), (7, 9), (9, 10), (10, 6), (6, 11), (11, 3)$$\n",
    "\n",
    "four of which have $x_t < x_{t+1}$, and two which have $x_t > x_{t+1}$. We can represent these permutations by listing the relative indicies of the values sorted smallest to largest.\n",
    "\n",
    "$$x_t < x_{t+1} => 01$$\n",
    "$$x_{t+1} < x_t => 10$$\n",
    "\n",
    "Permutation entropy of order $n = 2$ is a measure of the probabilities of the permutations 01 and 10.\n",
    "\n",
    "$$H(2) = -(4/6)log_2(4/6) - (2/6)log_2(2/6) \\approx 0.918$$\n",
    "\n",
    "Next, if you compare three consecutive values, there are five groups,\n",
    "\n",
    "$$(4, 7, 9), (7, 9, 10), (9, 10, 6), (10, 6, 11), (6, 11, 3)$$\n",
    "\n",
    "The possible permutations are\n",
    "\n",
    "$$x_t < x_{t+1} < x_{t+2} => 012$$\n",
    "$$x_t < x_{t+2} < x_{t+1} => 021$$\n",
    "$$x_{t+1} < x_t < x_{t+2} => 102$$\n",
    "$$x_{t+1} < x_{t+2} < x_t => 120$$\n",
    "$$x_{t+2} < x_t < x_{t+1} => 201$$\n",
    "$$x_{t+2} < x_{t+1} < x_t => 210$$\n",
    "\n",
    "There are two groups with the permutation 012, two with the permutation 201, and one with the permutation 102. The permutation entropy of order $n = 3$ is\n",
    "\n",
    "$$H(3) = -2 \\cdot (2/5)log_2(2/5) - (1/5)log_2(1/5) \\approx 1.522$$\n",
    "\n",
    "Notice that those permutations with no representation are not included in the equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition\n",
    "\n",
    "Consider a time series $\\{x_t\\}_{t=1..T}$. There are $n!$ possible permutations $\\pi$ of order $n$. For each $\\pi$, the relative frequency of $\\pi$ is\n",
    "\n",
    "$$p(\\pi) = \\frac{\\#\\{t | 0 \\le t \\le T - n, (x_{t+1},...,x_{t+n}) \\text{ has type } \\pi\\}}{T - n + 1}$$\n",
    "\n",
    "This estimates the frequency of $\\pi$ as closely as possible for a finite series of values.\n",
    "\n",
    "The *permutation entropy* of order $n \\ge 2$ is defined as\n",
    "\n",
    "$$H(n) = - \\sum p(\\pi)log_2p(\\pi)$$\n",
    "\n",
    "where the sum runs over all $n!$ permutations of $\\pi$ of order $n$.\n",
    "\n",
    "Entropy comes from Information Theory. It measures the average information associated with the outcome of an experiment.\n",
    "\n",
    "$$ H(X) = E \\left\\{I(x_j)\\right\\} = - \\sum_{j=1}^{n} p(x_j)log_2p(x_j)$$\n",
    "\n",
    "Where $p(x_j)$ is the probability of outcome $x_j$ and $-log_2p(x_j)$ is the amount of information in outcome $x_j$.\n",
    "\n",
    "So permutation entropy is a measure of the amount of information contained in the permutations of a signal.\n",
    "\n",
    "The *permutation entropy per symbol*, or *normalized permutation energy*, of order $n$ is found by dividing by $n - 1$ since comparison starts with the second value.\n",
    "\n",
    "$$h_n = \\frac{H(n)}{(n - 1)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Permutation Entropy Voice Activity Detector\n",
    "\n",
    "Permutation entropy (PE) evaluates the regularity or complexity of a signal. The complexity of voiced speech is low, unvoiced speech is mid-level, and noise is high. So given proper thresholds, permutation entropy can be expected to give good discrimination between speech and noise.\n",
    "\n",
    "To distinguish speech from noise, the energy:PE ratio is used.\n",
    "\n",
    "$$\\eta = \\frac{\\text{short time energy}}{h_n}$$\n",
    "\n",
    "If $\\eta$ exceeds some threshold $\\eta_{threshold}$, for some segment of a signal, that part of the signal is said to contain speech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our required libraries.\n",
    "\n",
    "`itertools` and `typing` are standard Python libraries. [Matplotlib](https://matplotlib.org/) is for pretty graphs. [Numpy](http://www.numpy.org/) is for fast numeric methods in Python. [Soundfile](https://github.com/bastibe/SoundFile) (pysoundfile) is for reading audio files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import typing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First up, energy of a signal is defined as\n",
    "\n",
    "$$E = \\sum_n{x_n^2}$$\n",
    "\n",
    "*Short time energy* is just the energy calculation over a segment of the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def energy(signal: np.ndarray) -> typing.Union[np.ndarray, np.dtype]:\n",
    "    \"\"\"Compute energy of signal.\n",
    "\n",
    "    Signal energy is the sum of the square of the values in the signal.\n",
    "\n",
    "    Args:\n",
    "        signal: input signal.\n",
    "\n",
    "    Returns:\n",
    "        signal energy.\n",
    "    \"\"\"\n",
    "    e = np.sum(np.square(signal))\n",
    "    return e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the *relative frequency* of a permutation is\n",
    "\n",
    "$$p(\\pi) = \\frac{\\#\\{t | 0 \\le t \\le T - n, (x_{t+1},...,x_{t+n}) \\text{ has type } \\pi\\}}{T - n + 1}$$\n",
    "\n",
    "and *permutation entropy* is\n",
    "\n",
    "$$H(n) = - \\sum p(\\pi)log_2p(\\pi)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def permutation_entropy(\n",
    "    signal: np.ndarray,\n",
    "    order: int\n",
    "):\n",
    "    \"\"\"Calculate the permutation entropy of signal.\n",
    "\n",
    "    As described in Bandt and Pompe.\n",
    "\n",
    "    Args:\n",
    "        signal: input signal.\n",
    "        order: number of samples in permutation. Must be >= 2.\n",
    "\n",
    "    Returns:\n",
    "        Permutation entropy of signal, a numpy scalar.\n",
    "    \"\"\"\n",
    "    # Permutations of indices.\n",
    "    permutation_type = [\n",
    "        list(p)\n",
    "        for p in itertools.permutations(range(order))\n",
    "    ]\n",
    "\n",
    "    relative_freqs = np.zeros(len(permutation_type))\n",
    "    permutation_samples = signal.size - order + 1\n",
    "\n",
    "    for idx in range(permutation_samples):\n",
    "        pi = np.argsort(signal[idx:idx + order]).tolist()\n",
    "        relative_freqs[permutation_type.index(pi)] += 1\n",
    "\n",
    "    relative_freqs /= permutation_samples\n",
    "\n",
    "    # Remove missing permutations.\n",
    "    relative_freqs = relative_freqs[relative_freqs != 0]\n",
    "\n",
    "    # Permutation entropy.\n",
    "    pe = -np.sum(relative_freqs * np.log2(relative_freqs))\n",
    "\n",
    "    return pe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Energy:PE ratio* is\n",
    "\n",
    "$$\\eta = \\frac{\\text{short time energy}}{h_n}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def energy_pe_ratio(\n",
    "    signal: np.ndarray,\n",
    "    order: int\n",
    ") -> typing.Union[np.ndarray, float]:\n",
    "    \"\"\"Calculate energy to PE ratio.\n",
    "\n",
    "    As described in Xu, Wang, and Bao.\n",
    "\n",
    "    Note: `order` is called `embedding factor` in the paper. We use `order`\n",
    "    because that is what is used in Bandt and Pompe. Plus it's shorter.\n",
    "\n",
    "    Args:\n",
    "        signal: input signal.\n",
    "        order: number of samples in permutation. Must be >= 2.\n",
    "\n",
    "    Returns:\n",
    "        energy:pe ratio.\n",
    "    \"\"\"\n",
    "    # Short time energy.\n",
    "    ste = energy(signal)\n",
    "\n",
    "    # Normalized permutation energy.\n",
    "    npe = permutation_entropy(signal, order) / (order - 1)\n",
    "\n",
    "    # Energy PE ratio.\n",
    "    eta = ste / npe if npe != 0.0 else 0.0\n",
    "\n",
    "    return eta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which finally brings us to the the permutation entropy voice activity detector. A `signal` is passed in along with its `sample_rate`. The `window_length` is the amount of signal, in seconds, to use in the energy:PE ratio calculation. `Threshold` is a multiplier over the noise floor to use for the threshold value. And `order` is the permutation order.\n",
    "\n",
    "The first window is assumed to contain no voice, and is used to determine the noise energy:PE ratio, $\\eta_{noise}$. This value is multiplied by `threshold` to determine the threshold energy:PE ratio, $\\eta_{threshold}$, against which the energy:PE ratio of the rest of the signal will be compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def entropy_vad_detailed(\n",
    "    signal: np.ndarray,\n",
    "    sample_rate: int,\n",
    "    window_length: float,\n",
    "    threshold: float,\n",
    "    order: int\n",
    ") -> typing.Tuple[\n",
    "    np.ndarray,\n",
    "    np.ndarray,\n",
    "    typing.Union[np.ndarray, float],\n",
    "    typing.Union[np.ndarray, float]\n",
    "]:\n",
    "    \"\"\"Permutation Entropy-based Voice Activity Detector.\n",
    "\n",
    "    Args:\n",
    "        signal: audio signal to VAD.\n",
    "        sample_rate: sample rate of signal in samples/second.\n",
    "        window_length: window size in seconds.\n",
    "        threshold: multiplier of noise energy PE ratio for energy PE threshold.\n",
    "        order: number of samples in permutation. Must be >= 2.\n",
    "\n",
    "    Returns:\n",
    "        Voice activity indicator,\n",
    "        energy PE ratio,\n",
    "        energy PE ratio noise (numpy scalar),\n",
    "        energy PE ratio threshold (numpy scalar)\n",
    "    \"\"\"\n",
    "    # Initialize values.\n",
    "    n_window_samples = int(window_length * sample_rate)\n",
    "    signal_eta = np.zeros_like(signal)\n",
    "    voice_activity = np.zeros_like(signal, dtype=np.bool_)\n",
    "\n",
    "    # Calculate baseline.\n",
    "    # First window is assumed to have no voice energy.\n",
    "    eta_noise = energy_pe_ratio(signal[:n_window_samples], order)\n",
    "    eta_threshold = threshold * eta_noise\n",
    "\n",
    "    # VAD signal.\n",
    "    for start in range(0, signal.size, n_window_samples):\n",
    "        end = start + n_window_samples\n",
    "        eta = energy_pe_ratio(signal[start:end], order)\n",
    "\n",
    "        signal_eta[start:end] = eta\n",
    "        voice_activity[start:end] = eta > eta_threshold\n",
    "\n",
    "    return (\n",
    "        voice_activity,\n",
    "        signal_eta,\n",
    "        eta_noise,\n",
    "        eta_threshold\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some visualization functions\n",
    "\n",
    "To help us see how well the permutation entropy vad works, some graphing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_signal(signal: np.ndarray, samplerate: int):\n",
    "    time = np.linspace(0, signal.size / samplerate, signal.size)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(time, signal)\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Amplitude')\n",
    "    ax.set_title('Signal')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_pe_vad(signal, samplerate, window, threshold, order):\n",
    "    activity, eta_signal, eta_noise, eta_threshold = entropy_vad_detailed(\n",
    "        signal,\n",
    "        samplerate,\n",
    "        window,\n",
    "        threshold,\n",
    "        order\n",
    "    )\n",
    "    time = np.linspace(0, signal.size / samplerate, signal.size)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(time, signal, label='signal')\n",
    "    ax.plot(time, activity, label='voice activity')\n",
    "    ax.plot(time, eta_signal / np.max(eta_signal), label='energy:pe (signal)')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Amplitude')\n",
    "    ax.set_title(f'Voice Activity, Window={window}ms, order={order}, threshold={eta_threshold:.3}')\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Knobs\n",
    "\n",
    "There are three ways to tune the permutation entropy VAD -- window size, threshold, and order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "window = 0.020   # in seconds.\n",
    "threshold = 1.2  # A multiple of the calculated noise energy. So > 1.0.\n",
    "order = 2        # Must be >= 2, and realistically <= 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Audio\n",
    "\n",
    "Our first test is on clean audio with lots of pauses between phrases. John F. Kennedy excelled at dramatic pauses, and his speeches are in the public domain. A very famous quote follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!play jfk_moon.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal, samplerate = sf.read('jfk_moon.wav')\n",
    "plot_signal(signal, samplerate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pe_vad(signal, samplerate, window, threshold, order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The yellow line indicates voice detected (1) or no voice detected (0). The actual value for the signal's energy:PE ratio is plotted in green, though it is scaled to fit on the graph, so does not represent the actual values. Window size, threshold, and order are all knobs you can play with. to smooth out the voice activity inidcator, or reduce false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noisy Audio\n",
    "\n",
    "Let's see how the entropy VAD holds up to some noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### White Noise\n",
    "\n",
    "White noise is the go-to additive noise in the signal processing world. It is uniformly distributed across the frequency range, which is great if you're trying to model a communications channel, but not so good if you're trying to model, say, a speaker talking above others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!play jfk_moon_white_15.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal, samplerate = sf.read('jfk_moon_white_15.wav')\n",
    "plot_signal(signal, samplerate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pe_vad(signal, samplerate, window, threshold, order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pink Noise\n",
    "\n",
    "Pink noise concentrates more of its energy in the lower frequencies. This is a better model for a speaker talking above others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!play jfk_moon_pink_15.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal, samplerate = sf.read('jfk_moon_pink_15.wav')\n",
    "plot_signal(signal, samplerate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pe_vad(signal, samplerate, window, threshold, order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brownian Noise\n",
    "\n",
    "Brownian noise concentrates even more of its energy in the lower frequencies than pink noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!play jfk_moon_brownian_15.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal, samplerate = sf.read('jfk_moon_brownian_15.wav')\n",
    "plot_signal(signal, samplerate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pe_vad(signal, samplerate, window, threshold, order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Permutation entropy is a viable, lightweight measure for voice activity detection. Its performance in noise is as good, or better than contemporary techniques, without the computation penalty. As such, it is suitable for use in real-time applications and on low-power platforms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "C. Bandt, B. Pompe, \"Permutation entropy -- a natural complexity measure for time series\", Physical Review Letters, vol 88, no 17, pp. 174102.1 - 174102.4, 2002.\n",
    "\n",
    "N. Xu, C. Wang, J. Bao, \"Voice Activity Detection Using Entropy-Based Method\", in ICSPCS, 2015.\n",
    "\n",
    "R. Ziemer, W. Tranter, \"Principles of Communication, Systems, Modulation, and Noise, Third Edition\", Boston: Houghton Mifflin, 1990. pp. 677-678.\n",
    "\n",
    "J. Kennedy, \"Special Message to the Congress on Urgent National Needs\", Washington D.C., 1961. Available at https://archive.org/details/jfks19610525"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
